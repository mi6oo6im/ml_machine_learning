{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Introduction to Machine Learning and Artificial Intelligence (August - September 2024)**\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "![alt text](image.png)\n",
    "\n",
    "**Lecturer:** Dr. Darshan Ingle\n",
    "\n",
    "**Modules Covered:**\n",
    "Matplotlib (matplotlib), WordCloud (wordcloud), HuggingFace Transformers (transformers), FastText (fasttext), Numpy (numpy), SMOTE (imblearn.over_sampling.SMOTE), GloVe (glove-python), Keras API (tensorflow.keras), NLTK (nltk), Seaborn (seaborn), Keras (tensorflow.keras), TQDM (tqdm), TensorFlow (tensorflow), Pandas (pandas), Scikit-learn (sklearn)\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "# Day 3: Classical Machine Learning: Data Classification Models. Unsupervised Learning (K-means Clustering)\n",
    "\n",
    "## Key Concepts:\n",
    "**1. Hyperparameter Tuning:**\n",
    "* **GridSearchCV:** Searches exhaustively over a specified parameter grid to find the best combination.\n",
    "* **RandomizedSearchCV:** Samples a fixed number of parameter combinations from a specified distribution, making it faster than GridSearchCV.\n",
    "\n",
    "**2. Classification Algorithms:**\n",
    "* Focus on predicting categorical outcomes (e.g., whether a passenger survived on the Titanic).\n",
    "\n",
    "**3. Metrics:**\n",
    "* Evaluating model performance based on various metrics.\n",
    "\n",
    "\n",
    "## Titanic Dataset:\n",
    "**1. Objective:**\n",
    "* Predict passenger survival based on features from the dataset.\n",
    "\n",
    "**2. Data Preparation:**\n",
    "* **Handling Missing Values:**\n",
    "    * fillna() method in Pandas: Fills missing values in the dataset.\n",
    "    * Use mode() for categorical data (most frequent value).\n",
    "    * For numerical data:\n",
    "        * If data is normally distributed, use the mean.\n",
    "        * For skewed data, use the median.\n",
    "* **Skewness:**\n",
    "    * Data with skewness between -0.5 and 0.5 is almost symmetrical, so filling missing values with the mean is reasonable.\n",
    "    * For positively or negatively skewed data, use the median.\n",
    "\n",
    "**3. Feature Engineering:**\n",
    "* **Family Size:** Creating new features from existing data.\n",
    "* **Get Dummies:** Convert categorical variables into dummy/indicator variables using pd.get_dummies().\n",
    "    * **Drop First:** Avoids multicollinearity by dropping the first dummy variable.\n",
    "\n",
    "**4. Normalization:**\n",
    "* **Standard Scaler:** Normalizes data by scaling features to have a mean of 0 and a standard deviation of 1.\n",
    "\n",
    "**5. Modeling:**\n",
    "* **Logistic Regression:**\n",
    "    * **Accuracy Score:** Proportion of correctly classified instances.\n",
    "    * **Confusion Matrix:** A table showing the performance of a classification model, detailing false positives, false negatives, true positives, and true negatives.\n",
    "        * **False Positive (FP):** Incorrectly predicted positive.\n",
    "        * **False Negative (FN):** Incorrectly predicted negative.\n",
    "        * **True Positive (TP):** Correctly predicted positive.\n",
    "        * **True Negative (TN):** Correctly predicted negative.\n",
    "    * **Accuracy:** (TP + TN) / N where N is the total number of samples, useful when data is balanced.\n",
    "    * **Sensitivity (Recall):** TP / (TP + FP), measures the proportion of actual positives correctly identified.\n",
    "    * **Specificity:** TN / (TN + FP), measures the proportion of actual negatives correctly identified.\n",
    "    * **F1 Score:** Harmonic mean of precision and recall, useful for imbalanced datasets.\n",
    "\n",
    "**6. Decision Tree Classifier:**\n",
    "* A model that splits data into branches to make predictions based on feature values.\n",
    "\n",
    "**7. CatBoost Classifier:**\n",
    "* A gradient boosting algorithm that handles categorical features well.\n",
    "\n",
    "## Unsupervised Learning:\n",
    "**1. Clustering:**\n",
    "* **Objective:** Grouping unlabeled data into clusters based on similarity.\n",
    "* **Centroid:** The average of all points in a cluster.\n",
    "\n",
    "**2. K-means Clustering:**\n",
    "* **Euclidean Distance:** Measures the distance between points and centroids.\n",
    "* **Iteration:** Algorithm iterates until cluster assignments no longer change.\n",
    "* **Shopping Mall Example:** Using clustering to segment customers based on their shopping behaviors.\n",
    "\n",
    "**3. Clustering Techniques:**\n",
    "* **Hopkins Test:** Determines if clustering is feasible by running 1000 iterations and checking the number of values greater than 0.7.\n",
    "* **Elbow Method:** Helps determine the optimal number of clusters by plotting the inertia (within-cluster variance) against the number of clusters.\n",
    "* **Silhouette Score:** Evaluates the cohesion and separation of clusters.\n",
    "* **Hierarchical Clustering:** Builds a hierarchy of clusters using either agglomerative (bottom-up) or divisive (top-down) methods.\n",
    "    * **Not Ideal for Categorical Data:** K-means may struggle with categorical features.\n",
    "\n",
    "**4. TQDM:**\n",
    "* Provides a progress bar for iterations, useful for monitoring long-running processes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Day 4 >>>](day_4.ipynb)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
