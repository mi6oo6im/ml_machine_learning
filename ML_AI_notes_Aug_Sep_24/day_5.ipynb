{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Introduction to Machine Learning and Artificial Intelligence (August - September 2024)**\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "# Day 5: Recurrent Neural Networks (RNN) and Large Language Models (LLM)\n",
    "## Recurrent Neural Networks (RNN):\n",
    "**1. Purpose:** \n",
    "* Designed to handle sequential data, such as time series or text.\n",
    "* **Activation Function:** Often use the tanh activation function to introduce non-linearity.\n",
    "\n",
    "**2. Sequence to Sequence Modeling:**\n",
    "* A type of RNN model used for tasks like translation or summarization where input and output are sequences.\n",
    "\n",
    "**3. Word Embeddings:**\n",
    "* Represent words as vectors in a continuous vector space, capturing semantic meanings.\n",
    "\n",
    "**4. Text Prerocessing:**\n",
    "* **Lowercasing:** Converts all characters to lowercase.\n",
    "* **Tokenization:** Splits text into words or tokens.\n",
    "* **Punctuation and Stop Words Removal:** Eliminates unnecessary punctuation and common words that might not contribute to meaning.\n",
    "* **Stemming and Lemmatization:** Reduces words to their base or root forms.\n",
    "* **Handling Contractions:** Expands contractions (e.g., “don’t” to “do not”).\n",
    "* **Emoji handling:**\n",
    "    * **Mapping Emojis:** Map emojis to textual representations to include them in text analysis.\n",
    "    * [Solutions ChatGPT](https://chatgpt.com/share/aa12a519-9b42-429c-b530-4771d62c2178)\n",
    "\n",
    "**5. NLTK Library:**\n",
    "* **Stem:** Provides stemming functionalities.\n",
    "\n",
    "**6. Sentiment Analysis Preprocessing:**\n",
    "* Involves cleaning and preparing text data for sentiment classification.\n",
    "\n",
    "**7. Count Vectorizer:**\n",
    "* Bag of Words Model: Represents text as the frequency of words. Not very powerful but simple.\n",
    "\n",
    "**8. TF-IDF (Term Frequency-Inverse Document Frequency):**\n",
    "* A more advanced technique that reflects how important a word is in a document relative to its frequency across all documents.\n",
    "\n",
    "**9. Sentiment Classification:**\n",
    "**Examples:** Analyzing movie reviews or restaurant feedback to classify sentiment.\n",
    "\n",
    "**10. The Vanishing Gradient Problem:**\n",
    "* A problem where gradients become very small during backpropagation, causing slow learning.\n",
    "\n",
    "**11. Long Short-Term Memory (LSTM):**\n",
    "* A type of RNN that can remember long-term dependencies by using memory cells to store information.\n",
    "\n",
    "**12. Attention Mechanism:**\n",
    "* Allows models to focus on different parts of the input sequence when making predictions.\n",
    "\n",
    "**13. Transformers:**\n",
    "* Models that handle entire sequences at once, addressing limitations of RNNs.\n",
    "\n",
    "14. ANM retention comparison:\n",
    "* **RNN:** Can remember up to 8 words.\n",
    "* **LSTM**: Can remember 50-100 words.\n",
    "* **Attention Mechanism:** Handles 1000 words.\n",
    "* **Transformers:** Efficiently process entire sequences.\n",
    "\n",
    "**15. Spam/Ham Classification example:**\n",
    "* Classification task to differentiate between spam and non-spam messages.\n",
    "\n",
    "**16. Handling Class Imbalance:**\n",
    "*   Techniques such as **SMOTE** (Synthetic Minority Over-sampling Technique), **MSMOTE** (Modified SMOTE), **KNN** (K-Nearest Neighbors), and **boosting**.\n",
    "\n",
    "**17. Keras Pad Sequences:**\n",
    "* Adds padding to sequences to ensure uniform length across all input sequences.\n",
    "\n",
    "**18. Early Stopping:**\n",
    "* Stops training when the model's performance on a validation set stops improving, based on hyperparameters like **min_delta** and **patience**.\n",
    "\n",
    "**19. Embedding:**\n",
    "* Represents categorical variables (like words) as dense vectors.\n",
    "\n",
    "**20. Sparse vs. Dense Matrices:**\n",
    "* **Sparse Matrix:** Contains mostly zeros, used to store large-scale data efficiently.\n",
    "* **Dense Matrix:** Contains mostly non-zero elements.\n",
    "\n",
    "**21. Word Embedding Models:**\n",
    "* **GloVe (Global Vectors for Word Representation):** Pretrained embeddings capturing word similarities.\n",
    "* **FastText:** Extends word embeddings to include subword information.\n",
    "* **GoogleNet and AlexNet:** Convolutional neural networks for image classification (not directly related to text, but influential in neural network development).\n",
    "\n",
    "## Transformers and HuggingFace:\n",
    "\n",
    "**1. HuggingFace:** \n",
    "* Provides pre-trained transformer models and tools.\n",
    "* [HuggingFace link](https://huggingface.co)\n",
    "* **Example:** Summarizing text using HuggingFace's models. Image generation. Translation.\n",
    "\n",
    "**2. DistilBART:** A distilled version of the BART model for text summarization.\n",
    "\n",
    "**3.Retrieval-Augmented Generation (RAG):**\n",
    "* Combines retrieval-based and generative approaches for more effective text generation.\n",
    "\n",
    "## Fine-Tuning Large Language Models (LLM):\n",
    "\n",
    "**1. OpenAI Playground:**\n",
    "* An interactive tool for experimenting with OpenAI's language models.\n",
    "* Adjusting pre-trained models to better fit specific tasks or datasets.\n",
    "* Retrieval-Augmented Generation\n",
    "\n",
    "## Future Topics:\n",
    "**1. Intro to GANs (Generative Adversarial Networks):** \n",
    "* Models for generating new data samples.\n",
    "\n",
    "**2. VAEs (Variational Autoencoders):** \n",
    "* Generative models that learn latent representations.\n",
    "\n",
    "**3. OpenAI API:** \n",
    "* API for interacting with OpenAI models.\n",
    "\n",
    "**4. Prompt Engineering:** \n",
    "* Designing effective prompts for LLMs.\n",
    "\n",
    "**5. LangChain:** \n",
    "* Framework for building applications with language models.\n",
    "\n",
    "**6. Other Models:** \n",
    "* Exploring models like LLaMA (Large Language Model Meta AI).\n",
    "\n",
    "**7. RAG:** \n",
    "* Retrieval-Augmented Generation"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
