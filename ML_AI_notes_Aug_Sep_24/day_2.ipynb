{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Introduction to Machine Learning and Artificial Intelligence (August - September 2024)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "![alt text](image.png)\n",
    "\n",
    "**Lecturer:** Dr. Darshan Ingle\n",
    "\n",
    "**Modules Covered:**\n",
    "Matplotlib (matplotlib), WordCloud (wordcloud), HuggingFace Transformers (transformers), FastText (fasttext), Numpy (numpy), SMOTE (imblearn.over_sampling.SMOTE), GloVe (glove-python), Keras API (tensorflow.keras), NLTK (nltk), Seaborn (seaborn), Keras (tensorflow.keras), TQDM (tqdm), TensorFlow (tensorflow), Pandas (pandas), Scikit-learn (sklearn)\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "# Day 2: Classical Machine Learning - Regression models\n",
    "\n",
    "## Historical Context:\n",
    "* **Machine Learning (ML) has been evolving since 1949.**\n",
    "* **\"Data is the new oil\" (2014):** Highlights the immense value of data in today's digital economy.\n",
    "* **AI, ML, DL:** Artificial Intelligence, Machine Learning, and Deep Learning are interrelated fields, with DL being a subset of ML, which itself is a subset of AI.\n",
    "\n",
    "## Key Concepts in ML:\n",
    "**1. Traditional vs. ML Paradigm:**\n",
    "* Traditional programming yields deterministic outputs (same input gives the same output), whereas ML models learn patterns and provide predictions.\n",
    "\n",
    "**2. Training and Testing Split:**\n",
    "* Machine Learning splits data into training (for learning) and testing (for evaluation).\n",
    "\n",
    "**3. ML = Predictive Analytics:**\n",
    "* The main objective of ML is to make predictions based on historical data.\n",
    "\n",
    "## Core Elements of Machine Learning:\n",
    "**1. Dependent Variable (Target) vs. Independent Variables (Features):**\n",
    "* Dependent variable is what you are trying to predict (y), while independent variables (X) are the inputs.\n",
    "\n",
    "**2. Data Splitting:**\n",
    "* X_train, X_test, y_train, y_test split into training and testing sets.\n",
    "* Correlation between independent variables is crucial for better model performance.\n",
    "\n",
    "**3. Continuous vs. Discrete Targets:**\n",
    "* In ML, target variables can be continuous (e.g., regression) or discrete (e.g., classification).\n",
    "\n",
    "## Supervised vs. Unsupervised Learning:\n",
    "**1. Supervised Learning:**\n",
    "* Uses labeled data with both X and y to make predictions.\n",
    "\n",
    "**2. Unsupervised Learning:**\n",
    "* Only uses input data X, without labeled output y.\n",
    "\n",
    "## Regression Models:\n",
    "**1. Linear Regression:**\n",
    "* Basic form: $y = mx + c$, where m is the slope and c is the y-intercept. The goal is to find the \"line of best fit\" that generalizes predictions.\n",
    "\n",
    "**2. Polynomial Regression:**\n",
    "* Extends linear regression by fitting a curve through the data points. $y = a_0 + a_1x + a_2x^2 + a_3x^3 + \\cdots + a_nx^n$<br>\n",
    "Where $a_0, a_1x, a_2$ are the coefficients of the polynomial terms.\n",
    "\n",
    "**3. Hyperplane:**\n",
    "* In higher dimensions (3+), regression models fit a plane or hyperplane instead of a line.\n",
    "\n",
    "**4. Risk in Extrapolation:**\n",
    "* Extrapolating beyond the range of data is risky and prone to errors.\n",
    "\n",
    "## Machine Learning Algorithms:\n",
    "**1. Regression Algorithms:**\n",
    "* **Linear Regression:** Simple and widely used but limited for non-linear relationships.\n",
    "* **Decision Trees:** More complex but offer higher accuracy.\n",
    "* **Random Forest:** A collection of decision trees for better generalization (n_estimators=100 by default).\n",
    "* **XGBoost:** Optimized version of boosting algorithms for high performance.\n",
    "\n",
    "**2. Feature Selection:**\n",
    "* **Recursive Feature Elimination (RFE):** Iteratively removes less important features.\n",
    "* **RFECV:** Cross-validated version of RFE to improve model reliability.\n",
    "\n",
    "## Model Evaluation Metrics:\n",
    "**1. Metrics:**\n",
    "* **Mean Absolute Error (MAE):** Average magnitude of errors. \n",
    "$$\\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} | y_i - \\hat{y}_i |$$\n",
    "* **Mean Squared Error (MSE):** Penalizes larger errors more than MAE.\n",
    "$$\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$$\n",
    "* **Root Mean Squared Error (RMSE):** Square root of MSE; commonly used in regression models.\n",
    "$$\\text{RMSE} = \\sqrt{ \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 }$$\n",
    "* **RÂ² Score:** Measures the proportion of variance explained by the model, where a value closer to 1 is better (0.8 is industry standard).\n",
    "$$R^2 = 1 - \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{n} (y_i - \\bar{y})^2}$$\n",
    "* **Adjusted RÂ²:** Adjusts RÂ² for the number of predictors, accounting for their significance.\n",
    "$$\\text{Adjusted } R^2 = 1 - \\frac{(1 - R^2)(n - 1)}{n - p - 1}$$\n",
    "\n",
    "Where:\n",
    "* $y_i = $ Actual value\n",
    "* $\\hat{y}_i = $ Predicted value\n",
    "* $\\bar{y} = $ Mean of actual values\n",
    "* $n = $ Number of data points\n",
    "* $p = $ Number of predictors in the model\n",
    "\n",
    "2. Industry standard: \n",
    "* Testing accuracy  $>= 80\\%$ (Prevent Underfitting)\n",
    "* $(A_{training} - A_{testing}) < 5\\%$ (Prevent Overfitting)\n",
    "\n",
    "## Train/Test Split and Scaling:\n",
    "**1 Train/Test Split:**\n",
    "* Commonly 80% training and 20% testing.\n",
    "* **Shuffle=True** by default to avoid bias.\n",
    "\n",
    "**2. Scaling and Normalization:**\n",
    "* **MinMax Scaler:** Rescales features to a range [0, 1].\n",
    "$$X_{\\text{scaled}} = \\frac{X - X_{\\text{min}}}{X_{\\text{max}} - X_{\\text{min}}}$$\n",
    "* **Standard Scaler (Z-score):** Centers around 0 with a standard deviation of 1. This is often preferred over MinMax scaling.\n",
    "$$Z = \\frac{X - \\mu}{\\sigma}$$\n",
    "Where ðœ‡ is the mean and Ïƒ is the standard deviation of the dataset.\n",
    "\n",
    "## Advanced Techniques:\n",
    "**1. Hyperparameter Tuning:**\n",
    "* Adjusting model parameters to optimize performance using techniques like GridSearchCV.\n",
    "\n",
    "**2. Support Vector Regressor (SVR):**\n",
    "* Used for both regression and classification with high accuracy, especially in complex datasets.\n",
    "\n",
    "## Advanced Visualization and Exploration:\n",
    "**1. Seaborn:**\n",
    "* **sns.pairplot():** Visualizes relationships between pairs of features.\n",
    "* **sns.heatmap():** Displays correlation matrices with color-coding.\n",
    "\n",
    "**2. Multicollinearity:**\n",
    "* Detects high correlation between independent variables, which can affect model performance.\n",
    "* Use **Pearson's correlation** coefficient for analysis.\n",
    "\n",
    "## Common Pitfalls in ML:\n",
    "**1. Overfitting:**\n",
    "* When a model performs too well on training data but poorly on testing data (train-test score difference > 0.5).\n",
    "\n",
    "**2. Underfitting:**\n",
    "* When the model fails to capture underlying patterns (train score < 0.8)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
