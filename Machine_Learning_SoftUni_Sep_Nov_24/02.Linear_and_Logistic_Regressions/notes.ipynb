{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Date: 17/09/2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Day 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear and Logistic Regressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SoftUni Summary\n",
    "[Link](https://softuni.bg/trainings/resources/officedocument/104338/material-summary-machine-learning-september-2024/4488)\n",
    "\n",
    "## Machine Learning Basics\n",
    "1. **Occam's Razor**: The principle that suggests preferring the simpler of two models or explanations. In machine learning, it implies choosing models that generalize better with fewer parameters, reducing the risk of overfitting.\n",
    "\n",
    "## Linear Regression\n",
    "1. **Linear Models**: These models are lightweight, fast, and still effective in many applications, especially when there’s a linear relationship between the features and the target variable.\n",
    "2. **Use Case**: Linear regression works best when the independent variable (X) has a linear correlation with the dependent variable (y).\n",
    "3. **Matrix Notation**: \n",
    "   $$\\mathbf{a^T X}$$\n",
    "   represents the linear equation for prediction, where $( \\mathbf{a} )$ is the vector of coefficients.\n",
    "4. **Loss Function**: The error for each prediction is:\n",
    "   $$d_i = (y_i - \\tilde{y}_i)^2$$\n",
    "   where $(\\tilde{y}_i )$ is the predicted value, and $( y_i )$ is the actual value.\n",
    "5. **Total Cost Function (J)**: \n",
    "   $$J = \\frac{1}{n} \\sum (y_i - \\hat{y}_i)^2$$\n",
    "\n",
    "   This function measures the overall model error. Mean Squared Error (MSE)\n",
    "6. **Objective**: We aim to find parameters $( a )$ and $( b )$ that minimize $( J )$ to achieve the most accurate predictions.\n",
    "7. **Loss Function Insight**: It evaluates how well the model’s predictions align with the actual data, acting as a gauge of model accuracy.\n",
    "8. **Optimizer / Solver**: A mathematical function used to minimize the cost function $( J )$, thereby improving model performance.\n",
    "9. **Gradient Descent**: A popular optimization algorithm that minimizes the cost function by adjusting the model parameters.\n",
    "10. **Constant Gradients**: For some variables, the gradient may be zero, indicating no change.\n",
    "11. **Gradient**: It’s a multidimensional derivative, representing the slope of the cost function with respect to each model parameter.\n",
    "12. **Saddle Point / Inflex Point**: A point where the gradient is zero but doesn’t indicate a maximum or minimum.\n",
    "13. **Global vs. Local Minimum**: In linear regression, the global and local minima are the same, simplifying optimization.\n",
    "14. **Gradient Descent Process**: It moves the model parameters in the direction of the steepest descent to find the global minimum.\n",
    "15. **Learning Rate $( \\alpha )$**: The step size that dictates how large or small the steps in the gradient descent process are.\n",
    "16. **Parameters and Hyperparameters**: Parameters are learned from the data, while hyperparameters are set manually (e.g., learning rate).\n",
    "17. **Scikit-Learn Datasets**: Built-in datasets used for model training and evaluation.\n",
    "18. **Demo: California Housing Dataset**: Example of using regression models on real-world housing data.\n",
    "19. **Scaling Features**: Techniques like `MinMaxScaler` and `StandardScaler` normalize features for better model performance.\n",
    "20. **Fit and Transform**: Use `fit_transform` on training data to learn scaling parameters and apply `transform` to test or future data.\n",
    "21. **Geospatial Features**: `geopandas` can be used for feature engineering by incorporating geographical data like latitude/longitude.\n",
    "22. **Model Coefficients**: Use `model.coef_` to assess each feature’s contribution to the target variable.\n",
    "23. **R² Score**: A statistical measure of how well the regression predictions approximate the real data points.\n",
    "24. **QR Decomposition**: An alternative to optimization for solving linear systems in regression.\n",
    "25. **Dealing with Outliers**: Outliers can skew linear regression models.\n",
    "26. **Outliers vs. Anomalies**: Outliers are data points that deviate significantly, while anomalies may indicate errors or rare events.\n",
    "27. **RANSAC (Random Sample Consensus)**: A robust method to fit models in the presence of outliers.\n",
    "28. **Inliers vs. Outliers**: Use `ransac_model.inlier_mask_` to identify which data points fit the model (inliers) versus those that don't (outliers).\n",
    "29. **Polynomial Regression**: A type of regression that fits a nonlinear relationship by introducing polynomial features.\n",
    "30. **Curse of Dimensionality**: As the number of features increases, the model’s performance may degrade due to data sparsity.\n",
    "\n",
    "## Logistic Regression\n",
    "1. **Binary Classification**: Logistic regression is primarily used for binary classification problems where the output is either 0 or 1.\n",
    "2. **Logistic Regression Equation**: Derived from linear regression, but the output is passed through a logistic (sigmoid) function to produce probabilities.\n",
    "3. **Generalized Linear Model (GLM)**: Uses the sigmoid function to map the continuous output from linear regression into probabilities:\n",
    "   $$\\sigma(z) = \\frac{1}{1 + e^{-z}}$$\n",
    "4. **Quantization**: Logistic regression quantizes the continuous output into binary predictions (0 or 1).\n",
    "5. **Why Sigmoid over Step Function**: Sigmoid provides smooth transitions between 0 and 1, whereas the step function jumps abruptly between 0 and 1, making it unsuitable for gradient-based optimization.\n",
    "6. **Loss Function**: Uses cross-entropy loss, which measures the distance between the predicted probability distribution and the actual labels:\n",
    "* for binary classification:\n",
    "   $$ \\mathcal{L} = - \\frac{1}{n} \\sum_{i = 1}^n \\left[ y_i \\log(\\tilde{y}_i) + (1 - y_i) \\log(1 - \\tilde{y}_i) \\right] $$\n",
    "   Where:\n",
    "- $( n )$ is the number of samples\n",
    "- $( y_i )$ is the true label (either 0 or 1)\n",
    "- $( \\tilde{y}_i )$ is the predicted probability for the positive class\n",
    "- $( \\log )$ is the natural logarithm\n",
    "* for multi-class classification:\n",
    "   $$ \\mathcal{L} = - \\frac{1}{n} \\sum_{i=1}^{n} \\sum_{j=1}^{k} y_{i,j} \\log(\\tilde{y}_{i,j}) $$\n",
    "\n",
    "Where:\n",
    "- $( k )$ is the number of classes\n",
    "- $( y_{i,j} )$ is the true label for class $( j )$\n",
    "- $( \\tilde{y}_{i,j} )$ is the predicted probability for class $( j )$\n",
    "\n",
    "7. **Demo: MNIST Dataset**: An example of applying logistic regression to the famous MNIST handwritten digits dataset.\n",
    "8. **Multiclass Classification**: By using one logistic regression for each digit (10 classes), the model classifies which digit is represented by the input.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
