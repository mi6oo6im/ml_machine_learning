{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Day 3\n",
    "\n",
    "## Model Training and Improvement\n",
    "\n",
    "- **Homework**: Review and summarize two classic ML articles with peer review. Reproduce the results of at least one, with at least one article published after January 1, 2021.\n",
    "- **Quote**: \"Machine Learning is the process we follow to get the right approximators that work in practice.\" — Yordan\n",
    "\n",
    "---\n",
    "\n",
    "## Bias-Variance Tradeoff\n",
    "\n",
    "1. **Diabetes Dataset Demo**: A walkthrough of model training using the Diabetes dataset.\n",
    "2. **Create Pipeline**: Use `sklearn.pipeline` to automate processes.\n",
    "3. **Pitfall**: Always ensure the same scaling is applied to both train/test data and any new data in production.\n",
    "4. **One-Hot Encoding**: Using `pd.get_dummies()` to encode categorical variables.\n",
    "5. **Pipeline**: A combination of preprocessing (e.g., scaling, encoding) and model fitting.\n",
    "6. **Components**: A pipeline generally consists of a **scaler**, **one-hot encoder**, and a **model**.\n",
    "7. **Sample Attributes and Target**: Separate feature and target columns in the dataset.\n",
    "8. **Preprocessor**: Use `sklearn.ColumnTransformer()` to preprocess specific columns.\n",
    "9. **FunctionTransformer**: Apply transformations via `sklearn.FunctionTransformer()` to your pipeline.\n",
    "10. **Nesting Pipelines**: Preprocessors and estimators can be nested within one pipeline.\n",
    "11. **Saving Pipelines**: Use the `pickle` library to dump/load the pipeline for future use.\n",
    "12. **Prediction**: Once trained, the pipeline can predict new data with consistent preprocessing.\n",
    "13. **No Perfect Model**: 100% accuracy is unattainable in real-world data due to inherent errors.\n",
    "14. **Irreducible Error (Noise)**: Also called the **Bayesian optimal error**, it represents data noise.\n",
    "15. **Variance**: This represents unpredictable statistical error due to external factors.\n",
    "16. **Bias**: Predictable errors caused by the wrong assumptions during model training.\n",
    "17. **Bias-Variance Tradeoff**: The balance between underfitting (high bias) and overfitting (high variance).\n",
    "18. **Underfitting vs Overfitting**: Underfitting fails to capture the underlying trend, while overfitting performs well on the training data but poorly on unseen data.\n",
    "19. **Not Always a Tradeoff**: Sometimes, adding regularization can improve both bias and variance.\n",
    "20. **Optimal Model**: The ideal model strikes a balance, often achieved with bias, and uses methods like regularization to prevent overfitting.\n",
    "\n",
    "---\n",
    "\n",
    "## Applying Regularization\n",
    "\n",
    "1. **Regularization**: A method to find a better bias-variance tradeoff by controlling the model's complexity.\n",
    "2. **Weight Coefficients**: These are added to the loss function to penalize complexity.\n",
    "3. **Lambda (λ)**: Determines the importance of regularization in the model.\n",
    "4. **L2 Regularization**: Uses the second norm (Ridge Regression), penalizing large weights.\n",
    "5. **L1 Regularization**: Uses the first norm (Lasso Regression), forcing some weights to zero.\n",
    "6. **ElasticNet**: Combines both L1 and L2 regularization for a balanced approach.\n",
    "7. **No Regularization in Linear Regression**: This is often due to historical reasons.\n",
    "8. **Lasso, Ridge, ElasticNet**: Popular methods for regularization. Lasso (L1), Ridge (L2), and ElasticNet (combination of both).\n",
    "9. **Logistic Regression**: A large `C` value (e.g., `1e12`) means little or no regularization.\n",
    "\n",
    "---\n",
    "\n",
    "## Training and Testing\n",
    "\n",
    "1. **Train-Test Split**: Generally, a 70/30 split is used to partition the data.\n",
    "2. **Test on Unseen Data**: Never test on the training data to avoid biased results.\n",
    "3. **Randomized Samples**: Shuffle the data for better generalization.\n",
    "4. **Stratified Sampling**: For classification tasks, use `stratify=target` to ensure balanced class distribution.\n",
    "5. **Test Set Size**: The size of the test set matters, not the ratio. Choose a sufficient size for evaluation.\n",
    "6. **Specialized Test Sets**: Sometimes, specialized sets (e.g., emoji datasets for sentiment analysis) are necessary.\n",
    "7. **Model Performance**: Use `pipeline.score()` to evaluate model performance.\n",
    "8. **Multiple Metrics**: One metric is not enough. Use several to get a full picture.\n",
    "9. **Regression Metrics**: Metrics like **R²** are used to evaluate regression models.\n",
    "10. **Classification Metrics**: For classification models, use metrics like **accuracy** and the **classification report**.\n",
    "11. **Metrics Module**: Use `sklearn.metrics` to access a wide range of performance metrics.\n",
    "12. **Residual Score**: Measures the difference between predicted and actual values.\n",
    "13. **Confusion Matrix**: Useful for evaluating classification models.\n",
    "14. **ROC Curve**: Receiver Operating Characteristic curve for visualizing performance in binary classification.\n",
    "15. **Limitations of ROC**: For multiclass classification, we use a \"1 vs. all\" approach.\n",
    "16. **ROC Curve Clarification**: The ROC curve should not fall below the diagonal dashed line, as this would imply worse-than-random performance.\n",
    "17. **Learning and Validation Curves**: Useful tools for understanding model performance across training sizes and parameter values.\n",
    "\n",
    "---\n",
    "\n",
    "## Cross-Validation\n",
    "\n",
    "1. **Cross-Validation**: Split data into train, validation, and test sets to improve generalization.\n",
    "2. **K-Fold Cross-Validation**: Breaks the dataset into K parts and runs K separate training/testing cycles to ensure robustness.\n",
    "\n",
    "---\n",
    "\n",
    "## Model Tuning and Selection\n",
    "\n",
    "1. **Hyperparameter Tuning**: Optimize the model's parameters to improve performance.\n",
    "2. **Grid Search**: Use `GridSearchCV()` to exhaustively search through predefined parameter grids.\n",
    "3. **Parameter Grid**: Define a grid of hyperparameters for tuning.\n",
    "4. **Randomized Search**: Use `RandomizedSearchCV()` for a more efficient, random search of hyperparameters.\n",
    "5. **Best Params**: Select the best parameters based on cross-validation performance.\n",
    "6. **CV Results**: Cross-validation results are available to evaluate different hyperparameter settings.\n",
    "7. **Hyperopt**: An alternative to `GridSearchCV()` for more advanced search algorithms.\n",
    "8. **Optuna**: A tool designed for tuning hyperparameters of artificial neural networks (ANN).\n",
    "\n",
    "---\n",
    "\n",
    "## Feature Selection and Feature Engineering\n",
    "\n",
    "1. **Occam's Razor**: Simpler models are often better.\n",
    "2. **Feature Reduction**: Reduce the number of features to improve performance and avoid overfitting.\n",
    "3. **Focus on Relevant Features**: Only keep features that contribute to the model's predictive power.\n",
    "4. **Removing Irrelevant Features**: Remove any irrelevant or redundant features that do not help in prediction.\n",
    "5. **Regularization**: Helps by penalizing the complexity of the model, naturally reducing irrelevant features.\n",
    "6. **Dimensionality Reduction**: Techniques like PCA (Principal Component Analysis) can help reduce feature space.\n",
    "7. **Feature Engineering**: Creating new meaningful features from raw data, such as using **geopandas** for geospatial data or **clustering** techniques. This requires deep domain knowledge and expertise.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
