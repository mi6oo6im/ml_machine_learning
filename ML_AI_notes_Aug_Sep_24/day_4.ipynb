{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Introduction to Machine Learning and Artificial Intelligence (August - September 2024)**\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "![alt text](image.png)\n",
    "\n",
    "**Lecturer:** Dr. Darshan Ingle\n",
    "\n",
    "**Modules Covered:**\n",
    "Matplotlib (matplotlib), WordCloud (wordcloud), HuggingFace Transformers (transformers), FastText (fasttext), Numpy (numpy), SMOTE (imblearn.over_sampling.SMOTE), GloVe (glove-python), Keras API (tensorflow.keras), NLTK (nltk), Seaborn (seaborn), Keras (tensorflow.keras), TQDM (tqdm), TensorFlow (tensorflow), Pandas (pandas), Scikit-learn (sklearn)\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "# Day 4: Artificial Neural Networks (ANN) and Deep Learning. Convolutional Neural Networks (CNN)\n",
    "## Key Concepts:\n",
    "**1. Artificial Neural Networks:**\n",
    "* **Mimic Human Brain:** ANN models aim to simulate how the human brain processes information.\n",
    "* **Basic Flow:** Input → Processing → Output\n",
    "\n",
    "**2. Training Process:**\n",
    "* **Epoch:** One forward pass (propagation) + one backward pass (propagation) through the network. An epoch refers to a complete iteration over the entire training dataset.\n",
    "\n",
    "**3. Frameworks and Libraries:**\n",
    "* **TensorFlow:** A popular open-source library for building and training neural networks.\n",
    "* **Keras API:** A high-level API for building and training neural networks that runs on top of TensorFlow (included in TF 2.0).\n",
    "\n",
    "**4. Artificial Neuron (Perceptron):**\n",
    "* Developed in 1949, it is a basic unit of neural networks, inspired by biological neurons.\n",
    "\n",
    "**5. Example:**\n",
    "* Predicting whether to attend a Punjabi food festival based on weather, spouse’s preference, and availability of train/metro.\n",
    "\n",
    "**6. Mathematics of Neurons:**\n",
    "* **Thresholding:** An artificial neuron fires if the weighted sum of inputs exceeds a certain threshold \n",
    "$$ \\sum_{i} (X_i \\cdot w_i) \\geq \\text{threshold} $$\n",
    "$$ \\sum_{i} (X_i \\cdot w_i) - \\text{threshold} \\geq 0 $$\n",
    "* **Bias:** \n",
    "$$b = - \\text{threshold}$$\n",
    "* **Activation Decision:**\n",
    "$$\\begin{align*}\n",
    "\\text{If }(w \\cdot x) + b &\\leq 0, \\quad \\text{the output is \\textbf{false.}} \\\\\n",
    "\\text{If }(w \\cdot x) + b &> 0, \\quad \\text{the output is \\textbf{true.}}\n",
    "\\end{align*}$$\n",
    "\n",
    "**7. Activation Functions:**\n",
    "* **Early Activation:** Step activation function (discrete, obsolete).\n",
    "* **Current Activation Functions:**\n",
    "    * **Binary Classification:** sigmoid\n",
    "    * **Multiclass Classification:** softmax\n",
    "    * **Regression:** linear\n",
    "    * **Hidden Layers:**\n",
    "        * **ReLU** (Rectified Linear Unit)\n",
    "        * **Leaky ReLU**\n",
    "        * **ELU** (Exponential Linear Unit): Good for reducing bias.\n",
    "        * **GELU** (Gaussian Error Linear Unit)\n",
    "        * **SELU** (Scaled Exponential Linear Unit)\n",
    "        * **SiLU** (Sigmoid Linear Unit)\n",
    "    * **Tanh:** Hyperbolic tangent function, useful for NLP tasks with values in the range [-1, 1].\n",
    "    * **Mish:** Effective for computer vision tasks.\n",
    "* [TensorFlow Activation Functions](https://www.tensorflow.org/api_docs/python/tf/keras/activations)\n",
    "* [Keras Activation Functions](https://keras.io/api/layers/activations/#tanh-function)\n",
    "\n",
    "**8. Model Building:**\n",
    "* **Single Perceptron Neural Network Example:** Basic ANN with one neuron.\n",
    "* **Activation Function:** Sigmoid for binary classification. SoftMax for multiclass.\n",
    "* **Dense Layers:** Fully connected layers where each neuron is connected to every neuron in the previous layer.\n",
    "\n",
    "**9. Optimization:**\n",
    "* **Optimizers:**\n",
    "    * **Adam:** Adaptive Moment Estimation.\n",
    "    * **SGD (Stochastic Gradient Descent):** A method for optimizing the loss function.\n",
    "* **Loss Functions:**\n",
    "    * **Binary Crossentropy:** Used for binary classification.\n",
    "    * **Sparse Categorical Crossentropy:** Used for multi-class classification where labels are integers.\n",
    "\n",
    "**10. Training Process:**\n",
    "    * **Forward Pass:** Calculating output from the input data.\n",
    "    * **Backpropagation:** Adjusting weights based on the error from the forward pass.\n",
    "    * **1 Epoch:** Includes one forward pass and one backward pass.\n",
    "\n",
    "**11. Evaluation Metrics:**\n",
    "* **Accuracy:** Measure of how often the model's predictions are correct.\n",
    "* **Graphs:**\n",
    "    * **Loss/Validation Loss Graph:** Shows how the loss changes over epochs.\n",
    "    * **Accuracy/Validation Accuracy Graph:** Shows how the accuracy changes over epochs.\n",
    "* **r.history:** Object storing training metrics for plotting.\n",
    "\n",
    "**12. Output Handling:**\n",
    "* **Probability Output:** Neural networks often output probabilities (e.g., [0, 1] for binary classification).\n",
    "* **Rounding and Flattening:** Convert probabilities to class labels if necessary.\n",
    "\n",
    "**13. Saving and Defining Models:**\n",
    "* Saving: Save the model using a '.keras' extension.\n",
    "* Define SGD: Custom implementation if needed.\n",
    "* Pass model for Deployment.\n",
    "\n",
    "**14. MNIST Dataset:**\n",
    "* A classic dataset for digit recognition tasks.\n",
    "\n",
    "**15. Batch Size:**\n",
    "* Default is 32. \n",
    "* Refers to the number of samples processed before the model's internal parameters are updated.\n",
    "\n",
    "**16. Cluster Analysis:**\n",
    "* **Number of Neurons:** A common heuristic (approach to problem solving that employs a pragmatic method that is not fully optimized) is $2^n$ for hidden layers where n is the number of input features (peceptron count).\n",
    "\n",
    "**17. Softmax Activation:**\n",
    "* Used for multiclass classification to convert logits to probabilities and handle overfitting.\n",
    "\n",
    "**18. Dropout:** \n",
    "* A Simple Way to Prevent Neural Networks from Overfitting\n",
    "* [Paper](https://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf)\n",
    "\n",
    "**19. Transfer Learning:**\n",
    "* **Concept of Tranfer Learning:** a pretrained model reused or adapted to a different, but related task.\n",
    "* Image handling using Transfer Learning with pretrained models\n",
    "* **Example:** Image recognition using VGG16\n",
    "* **Preprocessing for VGG16:**\n",
    "    * Typical preprocessing steps to adapt data for the VGG16 model involves resizing and normalization.\n",
    "* [Pretrained Model Suggestions ChatGPT](https://chatgpt.com/share/1f3d3db9-5181-40c1-a29a-c981d77cf8ad)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
